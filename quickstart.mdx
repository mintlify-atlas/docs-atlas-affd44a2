---
title: Quickstart
description: Get up and running with MLX in minutes
---

This guide will help you get started with MLX by creating arrays, performing operations, and using function transformations.

## Creating arrays

Import `mlx.core` and create your first array:

```python
import mlx.core as mx

# Create arrays
a = mx.array([1, 2, 3, 4])
print(a.shape)  # [4]
print(a.dtype)  # int32

b = mx.array([1.0, 2.0, 3.0, 4.0])
print(b.dtype)  # float32
```

## Lazy evaluation

<Note>
Operations in MLX are lazy. Arrays are only computed when needed.
</Note>

You can force evaluation using `eval()`:

```python
c = a + b  # Not evaluated yet
mx.eval(c)  # Now evaluated
```

Arrays are automatically evaluated when you:
- Print them
- Access scalar values with `.item()`
- Convert to NumPy arrays

```python
c = a + b
print(c)  # Automatically evaluates
# array([2, 4, 6, 8], dtype=float32)

# Convert to NumPy
import numpy as np
np_array = np.array(c)  # Also evaluates
```

Learn more in the [Lazy Evaluation](/concepts/lazy-evaluation) guide.

## Basic operations

MLX supports standard array operations:

<CodeGroup>
```python Arithmetic
import mlx.core as mx

a = mx.array([1, 2, 3])
b = mx.array([4, 5, 6])

# Element-wise operations
c = a + b  # [5, 7, 9]
d = a * b  # [4, 10, 18]
e = mx.exp(a)  # Exponential
```

```python Broadcasting
import mlx.core as mx

a = mx.array([[1, 2, 3], [4, 5, 6]])
b = mx.array([10, 20, 30])

# Broadcasting
c = a + b
# [[11, 22, 33],
#  [14, 25, 36]]
```

```python Reductions
import mlx.core as mx

a = mx.array([[1, 2, 3], [4, 5, 6]])

# Reductions
total = mx.sum(a)  # 21
mean = mx.mean(a)  # 3.5
max_val = mx.max(a, axis=1)  # [3, 6]
```
</CodeGroup>

## Function transformations

MLX provides composable function transformations for automatic differentiation and vectorization.

### Automatic differentiation

Compute gradients with `grad()`:

```python
import mlx.core as mx

x = mx.array(0.0)

# Gradient of sin(x)
grad_sin = mx.grad(mx.sin)
print(grad_sin(x))  # 1.0 (cos(0))

# Second derivative
grad2_sin = mx.grad(mx.grad(mx.sin))
print(grad2_sin(x))  # -0.0 (-sin(0))
```

### Value and gradient

Compute both function value and gradient efficiently:

```python
import mlx.core as mx

def loss_fn(x):
    return mx.sum(x ** 2)

x = mx.array([1.0, 2.0, 3.0])

# Get both value and gradient
value, grad = mx.value_and_grad(loss_fn)(x)
print(value)  # 14.0
print(grad)   # [2.0, 4.0, 6.0]
```

### Vectorization

Vectorize functions with `vmap()`:

```python
import mlx.core as mx

def normalize(x):
    return x / mx.sum(x)

# Batch of vectors
batch = mx.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])

# Vectorize across batch dimension
batch_normalize = mx.vmap(normalize)
result = batch_normalize(batch)
# [[0.333, 0.667],
#  [0.429, 0.571],
#  [0.455, 0.545]]
```

### Composable transformations

Transformations can be composed arbitrarily:

```python
import mlx.core as mx

# Gradient of vectorized function
grad_vmap_fn = mx.grad(mx.vmap(some_function))

# Vectorized gradient
vmap_grad_fn = mx.vmap(mx.grad(some_function))

# Any combination works
mx.grad(mx.vmap(mx.grad(fn)))
```

## Multi-device execution

Operations can run on CPU or GPU without copying data:

```python
import mlx.core as mx

# Arrays live in unified memory
a = mx.array([1, 2, 3])

# Run on GPU
b = mx.exp(a, stream=mx.gpu)

# Run on CPU
c = mx.sin(a, stream=mx.cpu)

# No data transfer needed - unified memory!
```

Learn more in the [Unified Memory](/concepts/unified-memory) guide.

## Next steps

<CardGroup cols={2}>
  <Card title="Core Concepts" icon="lightbulb" href="/concepts/lazy-evaluation">
    Understand lazy evaluation, unified memory, and more
  </Card>
  <Card title="Examples" icon="code" href="/examples/linear-regression">
    See complete examples including neural networks
  </Card>
  <Card title="Python API" icon="book" href="/api/array">
    Explore the full API reference
  </Card>
  <Card title="Guides" icon="map" href="/guides/indexing">
    Learn about indexing, NumPy comparison, and more
  </Card>
</CardGroup>
